_Title:_ [In-context Reinforcement Learning with Algorithm Distillation](https://arxiv.org/abs/2210.14215) (ICLR 2023)

_Author:_ Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih (DeepMind)

**Abstract**

We propose Algorithm Distillation (AD), a method for distilling reinforcement
learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to
reinforcement learn as an across-episode sequential prediction problem. A dataset
of learning histories is generated by a source RL algorithm, and then a causal
transformer is trained by autoregressively predicting actions given their preceding
learning histories as context. Unlike sequential policy prediction architectures that
distill post-learning or expert sequences, AD is able to improve its policy entirely
in-context without updating its network parameters. We demonstrate that AD can
reinforcement learn in-context in a variety of environments with sparse rewards,
combinatorial task structure, and pixel-based observations, and find that AD learns
a more data-efficient RL algorithm than the one that generated the source data.

<p align="center">
      <img src="img/zhijie wang/1-1.png" width="800" />
</p>

The NLL loss (Eq. 6):

$$
\mathcal{L}(\theta) = -\sum_{n=1}^N\sum_{t=1}^{T-1}\log P_{\theta}(A=a_t^{(n)}|h_{t-1}^{(n)},o_t^{(n)}).
$$

<p align="center">
      <img src="img/zhijie wang/1-2.png" width="800" />
</p>



<br>By <i>Zhijie Wang</i><br>

---

