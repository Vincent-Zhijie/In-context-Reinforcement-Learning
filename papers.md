_Title:_ [In-context Reinforcement Learning with Algorithm Distillation](https://arxiv.org/abs/2210.14215) (ICLR 2023)

_Author:_ Michael Laskin, Luyu Wang, Junhyuk Oh, Emilio Parisotto, Stephen Spencer, Richie Steigerwald, DJ Strouse, Steven Stenberg Hansen, Angelos Filos, Ethan Brooks, maxime gazeau, Himanshu Sahni, Satinder Singh, Volodymyr Mnih (DeepMind)

**Abstract**

We propose Algorithm Distillation (AD), a method for distilling reinforcement
learning (RL) algorithms into neural networks by modeling their training histories with a causal sequence model. Algorithm Distillation treats learning to
reinforcement learn as an across-episode sequential prediction problem. A dataset
of learning histories is generated by a source RL algorithm, and then a causal
transformer is trained by autoregressively predicting actions given their preceding
learning histories as context. Unlike sequential policy prediction architectures that
distill post-learning or expert sequences, AD is able to improve its policy entirely
in-context without updating its network parameters. We demonstrate that AD can
reinforcement learn in-context in a variety of environments with sparse rewards,
combinatorial task structure, and pixel-based observations, and find that AD learns
a more data-efficient RL algorithm than the one that generated the source data.

<p align="center">
      <img src="img/zhijie wang/1-1.png" width="800" />
</p>

**Algorithm**

The NLL loss (Eq. 6):

$$
\mathcal{L}(\theta) = -\sum_{n=1}^N\sum_{t=1}^{T-1}\log P_{\theta}(A=a_t^{(n)}|h_{t-1}^{(n)},o_t^{(n)}).
$$

<p align="center">
      <img src="img/zhijie wang/1-2.png" width="800" />
</p>



<br>By <i>Zhijie Wang</i><br>

---

_Title:_ [Supervised Pretraining Can Learn In-Context Reinforcement Learning](https://arxiv.org/abs/2210.14215) (NeurIPS 2023)

_Author:_ Jonathan N. Lee, Annie Xie, Aldo Pacchiano, Yash Chandak, Chelsea Finn, Ofir Nachum, Emma Brunskill (Stanford University, Microsoft Research, Google DeepMind)

**Abstract**

Large transformer models trained on diverse datasets have shown a remarkable
ability to learn in-context, achieving high few-shot performance on tasks they were
not explicitly trained to solve. In this paper, we study the in-context learning capabilities of transformers in decision-making problems, i.e., reinforcement learning
(RL) for bandits and Markov decision processes. To do so, we introduce and study
Decision-Pretrained Transformer (DPT), a supervised pretraining method where
the transformer predicts an optimal action given a query state and an in-context
dataset of interactions, across a diverse set of tasks. This procedure, while simple,
produces a model with several surprising capabilities. We find that the pretrained
transformer can be used to solve a range of RL problems in-context, exhibiting both
exploration online and conservatism offline, despite not being explicitly trained to
do so. The model also generalizes beyond the pretraining distribution to new tasks
and automatically adapts its decision-making strategies to unknown structure. Theoretically, we show DPT can be viewed as an efficient implementation of Bayesian
posterior sampling, a provably sample-efficient RL algorithm. We further leverage
this connection to provide guarantees on the regret of the in-context algorithm
yielded by DPT, and prove that it can learn faster than algorithms used to generate
the pretraining data. These results suggest a promising yet simple path towards
instilling strong in-context decision-making abilities in transformers.

<p align="center">
      <img src="img/zhijie wang/2-1.png" width="800" />
</p>

**Algorithm**

Let $D_j=\lbrace(s_1,a_1,s_1',r_1'),\cdots,(s_j,a_j,s_j',r_j')\rbrace$. The expected loss (eq. 2):

$$
\min_{\theta}\mathbb{E}\_{P_{\text{pre}}}\sum_{j\in[n]}-\log M_{\theta}(a^*\mid s_{\text{query}},D_j).
$$

<p align="center">
      <img src="img/zhijie wang/2-2.png" width="800" />
</p>

**Theoretical analysis**

The equivalence of posterior sampling (PS) and Decision-Pretrained Transformer (DPT):

<p align="center">
      <img src="img/zhijie wang/2-3.png" width="800" />
</p>

<br>By <i>Zhijie Wang</i><br>

---

